lexer = import("lexer.sy")

ctx = {}

init = fn (buffer) {
    ctx.buffer = buffer
    ctx.tokens = []
    ctx.current_token_index = 0

    lexer.init(buffer)

    while true {
        token = lexer.next_token()

        array_push(ctx.tokens, token)

        if token.kind == lexer.token_kind.EOF {
            break
        }
    }
}

token_value = fn (token) {
    string_slice = fn (str, start, end) {
        new_str = ""

        i = start

        while i < end {
            new_str += str[i]

            i += 1
        }

        return new_str
    }

    return string_slice(ctx.buffer, token.buffer_loc.start, token.buffer_loc.end)
}

next_token = fn () {
    token = ctx.tokens[ctx.current_token_index]
    ctx.current_token_index += 1
    return token
}

peek_token = fn () {
    return ctx.tokens[ctx.current_token_index]
}

precedence_kind = {
    "LOWEST": 0,
    "SUM": 1,
    "PRODUCT": 2,
    "EXPONENT": 3,
}

precedence_from_token = fn (token) {
    map = {
        lexer.token_kind.PLUS: precedence_kind.SUM,
        lexer.token_kind.MINUS: precedence_kind.SUM,
        lexer.token_kind.FORWARD_SLASH: precedence_kind.PRODUCT,
        lexer.token_kind.STAR: precedence_kind.PRODUCT,
        lexer.token_kind.DOUBLE_STAR: precedence_kind.EXPONENT,
    }

    if !contains(map_keys(map), token.kind) {
        return precedence_kind.LOWEST
    } else {
        return map[token.kind]
    }
}

step_kind = {
    "NUMBER": 0,
    "BINARY_OPERATION": 1,
    "UNARY_OPERATION": 2,
}

parse = fn () {
    procedure = []

    while peek_token().kind != lexer.token_kind.EOF {
        step = parse_step(precedence_kind.LOWEST)

        if typeof(step) == "string" {
            return step
        }

        array_push(procedure, step)
    }

    return procedure
}

parse_step = fn (precedence) {
    lhs = parse_unary_expression()

    if typeof(lhs) == "string" {
        return lhs
    }

    while precedence < precedence_from_token(peek_token()) {
        lhs = parse_binary_expression(lhs)

        if typeof(lhs) == "string" {
            return lhs
        }
    }

    return lhs
}

is_unary_operator = fn (token) {
    if token.kind == lexer.token_kind.MINUS{
        return true
    } else {
        return false
    }
}

parse_unary_expression = fn () {
    if peek_token().kind == lexer.token_kind.NUMBER {
        return parse_number()
    } else if is_unary_operator(peek_token()) {
        return parse_unary_operation()
    } else {
        return "unexpected token"
    }
}

parse_number = fn () {
    value = to_float(token_value(next_token()))

    if value == none {
        return "invalid number"
    }

    return {
        "kind": step_kind.NUMBER,
        "value": value,
    }
}

unary_operator_kind = {
    "MINUS": 0,
}

unary_operator_from_token = fn (token) {
    map = {
        lexer.token_kind.MINUS: unary_operator_kind.MINUS,
    }

    return map[token.kind]
}

parse_unary_operation = fn () {
    operator = unary_operator_from_token(next_token())

    rhs = parse_step(precedence_kind.LOWEST)

    return {
        "kind": step_kind.UNARY_OPERATION,
        "operator": operator,
        "rhs": rhs,
    }
}


is_binary_operator = fn (token) {
    binary_operator_tokens = [
        lexer.token_kind.PLUS,
        lexer.token_kind.MINUS,
        lexer.token_kind.FORWARD_SLASH,
        lexer.token_kind.STAR,
        lexer.token_kind.DOUBLE_STAR,
    ]

    return contains(binary_operator_tokens, token.kind)
}

parse_binary_expression = fn (lhs) {
    if is_binary_operator(peek_token()) {
        return parse_binary_operation(lhs)
    } else {
        return "unexpected token"
    }
}

binary_operator_kind = {
    "PLUS": 0,
    "MINUS": 1,
    "FORWARD_SLASH": 2,
    "STAR": 3,
    "DOUBLE_STAR": 4,
}

binary_operator_from_token = fn (token) {
    map = {
        lexer.token_kind.PLUS: binary_operator_kind.PLUS,
        lexer.token_kind.MINUS: binary_operator_kind.MINUS,
        lexer.token_kind.FORWARD_SLASH: binary_operator_kind.FORWARD_SLASH,
        lexer.token_kind.STAR: binary_operator_kind.STAR,
        lexer.token_kind.DOUBLE_STAR: binary_operator_kind.DOUBLE_STAR,
    }

    return map[token.kind]
}

parse_binary_operation = fn (lhs) {
    operator = binary_operator_from_token(next_token())

    rhs = parse_step(precedence_kind.LOWEST)

    return {
        "kind": step_kind.BINARY_OPERATION,
        "lhs": lhs,
        "operator": operator,
        "rhs": rhs,
    }
}

export({
    "init": init,
    "parse": parse,
    "step_kind": step_kind,
    "binary_operator_kind": binary_operator_kind,
    "unary_operator_kind": unary_operator_kind,
})
